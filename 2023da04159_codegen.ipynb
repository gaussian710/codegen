{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07c8a36c-954a-44dd-bcf4-e5777d3f0f34",
   "metadata": {},
   "source": [
    "# Library setup\n",
    "- pip install numpy pandas sqlalchemy sqlglot\n",
    "- pip install torch transformers spacy \n",
    "- pip install Levenshtein accelerate bitsandbytes sentence-transformers spacy-transformers\n",
    "- python -m spacy download en_core_web_md\n",
    "- python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3bd4e6f-0ea8-42d8-a269-b11d46f394a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d9f934b-4f1d-4b87-8d6a-51135c45d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "import sqlglot\n",
    "import os, re, logging, pickle\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4452a8b-e317-4bc1-a025-94ece80dec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlglot import parse_one, exp, parse, table, column, to_identifier\n",
    "from Levenshtein import distance\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e109280-4464-4547-8726-72c761aa994b",
   "metadata": {},
   "source": [
    "## Query LLMs through Ollama\n",
    "This class is used to run hosted ollama models. see here for more details about Ollama (https://github.com/ollama/ollama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "866c76c2-6018-430d-82ff-9295fa62ecae",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_URL = 'http://127.0.0.1:9565'\n",
    "class OLLAMA:\n",
    "    def __init__(self, OLLAMA_URL, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.ollama_url = OLLAMA_URL\n",
    "        self.ollama_endpoint = '/api/generate'\n",
    "\n",
    "    def run(self, prompt):\n",
    "        data = {\n",
    "            'model': self.model_name,\n",
    "            'prompt': prompt,\n",
    "            'stream': False,\n",
    "            \"options\":{\"temperature\":0.1}\n",
    "        }\n",
    "\n",
    "        print(data)\n",
    "\n",
    "        headers = {\n",
    "            'Accept': 'application/json',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        resp = requests.post(url = f'{self.ollama_url}{self.ollama_endpoint}',\n",
    "                             data = json.dumps(data),\n",
    "                             headers = headers)\n",
    "        query = resp.json()['response']\n",
    "        print(f'JSON resp: {query}')\n",
    "        return query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ea325e-a3e1-4fa4-821a-ce36b7e31387",
   "metadata": {},
   "source": [
    "## Setup file database connection to fetch schema and execute queries\n",
    "SQLite database is used to load data csv into a file table. This table is then used to fetch descriptions and execute LLM generated queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "463e89ed-2666-4c57-9b44-1e7ac24ae271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTableDescriptionSQLiteDB(output_fmt):\n",
    "    engine = create_engine('sqlite:///mysqlitedb.db')\n",
    "    with engine.connect() as conn, conn.begin():\n",
    "        sqlite_master = pd.read_sql_query('SELECT * FROM sqlite_master', conn)\n",
    "    sqlite_master['sql_fmt'] = sqlite_master['sql'].apply(lambda z: [x.strip().strip(',').rsplit(' ', maxsplit=1) for x in z.split('\\n')[1:-1]])\n",
    "    table_desc_dict = {}\n",
    "    if output_fmt == 'df':\n",
    "        for _, row in sqlite_master.iterrows():\n",
    "            table_desc_dict[row['name']] = pd.DataFrame(columns=['name', 'type'], data=row['sql_fmt'])\n",
    "            table_desc_dict[row['name']]['comment'] = np.nan\n",
    "    elif output_fmt == 'ddl':\n",
    "        for _, row in sqlite_master.iterrows():\n",
    "            table_desc_dict[row['name']] = row['sql']\n",
    "    return table_desc_dict\n",
    "\n",
    "def getSQLiteDBQueryResult(query):\n",
    "    engine = create_engine('sqlite:///mysqlitedb.db')\n",
    "    try:\n",
    "        with engine.connect() as conn, conn.begin():\n",
    "            query_result = pd.read_sql_query(query, conn)\n",
    "    except Exception as e:\n",
    "        print(f'Error: {e}, {e.__traceback__}')\n",
    "        query_result = pd.DataFrame()\n",
    "    return query_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5097d87-2c1c-4e95-ae0a-46b428467902",
   "metadata": {},
   "source": [
    "## PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "190e823c-dd22-4cb3-aeae-72480a44ffd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_table(question, schema, table_name):\n",
    "    \"\"\"\n",
    "        This function generates embeddings for question and table schema and filters relevant columns for prompt creation\n",
    "        following the the filteration components:\n",
    "        1. Cosine embedding simlarity\n",
    "        2. NLP token matching between columns and question\n",
    "        3. Checking if question concerns date/time and adding date/time based columns\n",
    "    \"\"\"\n",
    "    column_embs, column_descriptions_typed = generate_embeddings(table_name, schema)\n",
    "\n",
    "    # 1a) get top k columns\n",
    "    top_k_scores, top_k_indices = knn_(question, column_embs, top_k=8, threshold=0.0)\n",
    "    topk_table_columns = {}\n",
    "    table_column_names = set()\n",
    "\n",
    "    for score, index in zip(top_k_scores, top_k_indices):\n",
    "        table_name, column_info = column_descriptions_typed[index].split('.', 1)\n",
    "        column_tuple = re.split(r',\\s*(?![^()]*\\))', column_info, maxsplit=2) # split only on commas outside parantheses\n",
    "        if table_name not in topk_table_columns:\n",
    "            topk_table_columns[table_name] = []\n",
    "        topk_table_columns[table_name].append(column_tuple)\n",
    "        table_column_names.add(f'{table_name}.{column_tuple[0]}')\n",
    "    \n",
    "    # 1b) get columns which match terms in question\n",
    "    nlp = spacy.load('en_core_web_trf')\n",
    "    question_doc = nlp(question)\n",
    "    q_filtered_tokens = [token.lemma_.lower() for token in question_doc if not token.is_stop]\n",
    "    q_alpha_tokens = [i for i in q_filtered_tokens if (len(i)>1 and i.isalpha())]\n",
    "\n",
    "    TIME_TERMS = ['when', 'time', 'hour', 'minute', 'second',\n",
    "                  'day', 'yesterday', 'today', 'tomorrow',\n",
    "                  'week', 'month', 'year',\n",
    "                  'duration', 'date']\n",
    "    \n",
    "    time_in_q = False\n",
    "\n",
    "    nlp_ner = spacy.load('en_core_web_md')\n",
    "    q_ner_doc = nlp_ner(question)\n",
    "    ent_types = [w.label_ for w in q_ner_doc.ents]\n",
    "\n",
    "    if 'DATE' in ent_types or 'TIME' in ent_types:\n",
    "        time_in_q = True\n",
    "    elif any([term in question.lower() for term in TIME_TERMS]):\n",
    "        time_in_q = True\n",
    "    elif set(q_alpha_tokens).intersection(set(TIME_TERMS)):\n",
    "        time_in_q = True\n",
    "    \n",
    "    for col_details in column_descriptions_typed:\n",
    "        table_name, column_info = col_details.split('.', 1)\n",
    "        column_tuple = re.split(r',\\s*(?![^()]*\\))', column_info, maxsplit=2) # split only on commas outside parantheses\n",
    "        col_name = column_tuple[0]\n",
    "\n",
    "        if column_tuple in topk_table_columns[table_name]:\n",
    "            continue\n",
    "\n",
    "        # if question concerns time, add time-related columns\n",
    "        if time_in_q and any([timetype in column_tuple[1] for timetype in ['DATE', 'TIMESTAMP']]):\n",
    "            if table_name not in topk_table_columns:\n",
    "                topk_table_columns[table_name] = []\n",
    "            if column_tuple not in topk_table_columns[table_name]:\n",
    "                topk_table_columns[table_name].append(column_tuple)\n",
    "            table_column_names.add(f'{table_name}.{column_tuple[0]}')\n",
    "            continue\n",
    "\n",
    "        # if question-token-lemmas overlap with column-token-lemmas, add the column\n",
    "        column_doc = nlp(col_name.replace('_', ' '))\n",
    "        col_tokens = [token.lemma_.lower() for token in column_doc if not token.is_stop]\n",
    "        col_alpha_tokens = [i for i in col_tokens if (len(i)>1 and i.isalpha())]\n",
    "        if set(col_alpha_tokens).intersection(set(q_alpha_tokens)):\n",
    "            if table_name not in topk_table_columns:\n",
    "                topk_table_columns[table_name] = []\n",
    "            if column_tuple not in topk_table_columns[table_name]:\n",
    "                topk_table_columns[table_name].append(column_tuple)\n",
    "            table_column_names.add(f'{table_name}.{column_tuple[0]}')\n",
    "\n",
    "    # 4) format metadata string\n",
    "    pruned_schema = format_topk_sql(topk_table_columns, shuffle=False)\n",
    "    print(f'Pruned schema: {pruned_schema}')\n",
    "    return pruned_schema\n",
    "\n",
    "def generate_embeddings(table_name, schema):\n",
    "    \"\"\"\n",
    "        Generate embedding for all the columns in the table schema\n",
    "    \"\"\"\n",
    "    num_cols = 0\n",
    "    TAB_DETAILS = []\n",
    "\n",
    "    for col in sqlglot.parse_one(schema, dialect='snowflake').find_all(sqlglot.exp.ColumnDef):\n",
    "        num_cols += 1\n",
    "        TAB_DETAILS.append([table_name, col.alias_or_name, col.find(sqlglot.exp.DataType).__str__(), col.find(sqlglot.exp.ColumnConstraint)])\n",
    "    \n",
    "    encoder = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1', device='cpu')\n",
    "\n",
    "    column_descriptions = []\n",
    "    column_descriptions_typed = []\n",
    "\n",
    "    for row in TAB_DETAILS:\n",
    "        tab_name, col_name, col_dtype, col_desc = row\n",
    "        col_str = f'{tab_name}.{col_name}:{col_desc}'\n",
    "        col_str_typed = f'{tab_name}.{col_name},{col_dtype},{col_desc}'\n",
    "        column_descriptions.append(col_str)\n",
    "        column_descriptions_typed.append(col_str_typed)\n",
    "\n",
    "    column_embs = encoder.encode(column_descriptions, convert_to_tensor=True, device='cpu')\n",
    "    return column_embs, column_descriptions_typed\n",
    "\n",
    "def knn_(query, all_embs, top_k, threshold):\n",
    "    \"\"\"\n",
    "        Find top k similar embedding using cosine simiarlity\n",
    "    \"\"\"\n",
    "    encoder = SentenceTransformer('mixedbread-ai/mxbai-embed-large-v1', device='cpu')\n",
    "    query_emb = encoder.encode(query, convert_to_tensor=True, device='cpu').unsqueeze(0)\n",
    "    similarity_scores = F.cosine_similarity(query_emb, all_embs)\n",
    "    top_results = torch.nonzero(similarity_scores > threshold).squeeze()\n",
    "\n",
    "    # if top_results is empty, return empty tensor\n",
    "    if top_results.numel() == 0:\n",
    "        return torch.tensor([]), torch.tensor([])\n",
    "    \n",
    "    # if only one result in resturned, convert to tensor\n",
    "    elif top_results.numel() == 1:\n",
    "        return torch.tensor([similarity_scores[top_results]]), torch.tensor([top_results])\n",
    "    \n",
    "    else:\n",
    "        top_k_scores, top_k_indices = torch.topk(similarity_scores[top_results], k=min(top_k, top_results.numel()))\n",
    "        return top_k_scores, top_results[top_k_indices]\n",
    "\n",
    "def format_topk_sql(topk_table_columns, shuffle):\n",
    "    \"\"\"\n",
    "        format top k columns back to original schema definition\n",
    "    \"\"\"\n",
    "    if len(topk_table_columns) == 0:\n",
    "        return ''\n",
    "    \n",
    "    md_str = '\\n'\n",
    "    # shuffle the keys in topk_table_columns\n",
    "    table_names = list(topk_table_columns.keys())\n",
    "    if shuffle:\n",
    "        np.random.seed(0)\n",
    "        np.random.shuffle(table_names)\n",
    "    for table_name in table_names:\n",
    "        columns_str = ''\n",
    "        columns = topk_table_columns[table_name]\n",
    "        if shuffle:\n",
    "            np.random.seed(0)\n",
    "            np.random.shuffle(columns)\n",
    "        for column_tuple in columns:\n",
    "            if len(column_tuple) > 2:\n",
    "                columns_str += (\n",
    "                    f'\\n  {column_tuple[0]} {column_tuple[1]}, --{column_tuple[2]}'\n",
    "                )\n",
    "            else:\n",
    "                columns_str += f'\\n  {column_tuple[0]} {column_tuple[1]}, '\n",
    "        md_str += f'CREATE TABLE {table_name} ({columns_str}\\n);\\n'\n",
    "    md_str += '\\n'\n",
    "    return md_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20b4f5d7-7706-430b-8445-60137a96e469",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abst'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 'absT'\n",
    "x.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0069c0-a62e-4247-9a8d-9436cc8300a1",
   "metadata": {},
   "source": [
    "## POSTPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92720dc2-7ebc-4173-9545-ac3cc308cc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class queryPostprocessing:\n",
    "\n",
    "    def __init__(self, query, table_metadata, embedding_model_name):\n",
    "        self.query = query.split(';')[0]\n",
    "        self.table_metadata = table_metadata\n",
    "        self.col_mapping = {}\n",
    "        self.table_mapping = {}\n",
    "        self.embedding_model = SentenceTransformer(embedding_model_name, device='cpu')\n",
    "    \n",
    "    def getIncorrectColumns(self):\n",
    "        \"\"\"\n",
    "            Get list of hallucinated or incorrect columns in the generated query\n",
    "        \"\"\"\n",
    "        column_names_query = [col.name.lower() for col in parse_one(self.query).find_all(exp.Column)]\n",
    "        column_names_query = np.unique(column_names_query).tolist()\n",
    "        column_names_table = [name.lower() for name in self.table_metadata['columns']['name'].tolist()]\n",
    "\n",
    "        matching_cols = list(set(column_names_query).intersection(set(column_names_table)))\n",
    "        invalid_cols = list(set(column_names_query).difference(set(matching_cols)))\n",
    "        available_set = list(set(column_names_table).difference(set(matching_cols)))\n",
    "\n",
    "        # check if enough columns are available to substitute (3 columns in query but only 2 in table)\n",
    "        for col in matching_cols:\n",
    "            self.col_mapping[col] = col\n",
    "        \n",
    "        for col in invalid_cols:\n",
    "            closest_col = self.embedding_distance(col, available_set)\n",
    "            self.col_mapping[col] = closest_col\n",
    "            available_set.remove(closest_col)\n",
    "        \n",
    "        return self.col_mapping\n",
    "    \n",
    "    def getIncorrectTables(self):\n",
    "        \"\"\"\n",
    "            Get list of hallucinated or incorrect tables in the LLM generated query\n",
    "        \"\"\"\n",
    "        ast = parse_one(self.query)\n",
    "        table_list = []\n",
    "        for tbl in ast.find_all(exp.Table):\n",
    "            tbl.set('alias',None)\n",
    "            table_name = tbl.sql()\n",
    "            table_list.append(table_name)\n",
    "        \n",
    "        unique_tables = np.unique(table_list).tolist()\n",
    "        if len(unique_tables):\n",
    "            table_name = unique_tables[0]\n",
    "            self.table_mapping[table_name] = self.table_metadata['table_name']\n",
    "        \n",
    "        return self.table_mapping\n",
    "    \n",
    "    def lv_distance(self, col, available_set):\n",
    "        \"\"\"\n",
    "            Compute levenshtein distance metric between query columns and table schema columns to derive most similar column name to impute in the generated query\n",
    "        \"\"\"\n",
    "        distances = []\n",
    "        for column in available_set:\n",
    "            d = distance(col, column)\n",
    "            distances.append(d)\n",
    "        most_similar_column = available_set[np.argmin(distances)]\n",
    "        return most_similar_column\n",
    "    \n",
    "    def embedding_distance(self, col, available_set):\n",
    "        \"\"\"\n",
    "            Compute cosine similiarty distance metric between query columns embeddings and table schema columns embeddings to derive most \n",
    "            similar column name to impute in the generated query\n",
    "        \"\"\"\n",
    "        col_embedding = self.embedding_model.encode([col])\n",
    "        available_set_embedding = self.embedding_model.encode(available_set)\n",
    "        similarity_list = cosine_similarity(col_embedding, available_set_embedding)\n",
    "        most_similar_column = available_set[np.argmax(similarity_list)]\n",
    "        return most_similar_column\n",
    "    \n",
    "    def formatQuery(self):\n",
    "        _ = self.getIncorrectColumns()\n",
    "        _ = self.getIncorrectTables()\n",
    "        updated_query = self.query\n",
    "        \n",
    "        for tbl, updated_tbl in self.table_mapping.items():\n",
    "            updated_query = updated_query.replace(tbl, updated_tbl)\n",
    "        \n",
    "        for col, updated_col in self.col_mapping.items():\n",
    "            updated_query = updated_query.replace(col, updated_col)\n",
    "        \n",
    "        return updated_query\n",
    "    \n",
    "    def formatQuerySQLglot(self):\n",
    "        \"\"\"\n",
    "            Parse the LLM generated query to fetch incorrect column and table names, and impute most similar objects from table schema\n",
    "        \"\"\"\n",
    "        _ = self.getIncorrectColumns()\n",
    "        _ = self.getIncorrectTables()\n",
    "        query_ast = parse_one(self.query)\n",
    "        alias_cols = [al.alias for al in query_ast.find_all(exp.Alias)]\n",
    "\n",
    "        for col in alias_cols:\n",
    "            self.col_mapping.pop(col, None)\n",
    "        \n",
    "        for tbl in query_ast.find_all(exp.Table):\n",
    "            table_alias = None\n",
    "            if 'alias' in tbl.args:\n",
    "                table_alias = tbl.alias\n",
    "            tbl.set('alias',None)\n",
    "            table_name = tbl.sql()\n",
    "            if table_name in self.table_mapping:\n",
    "                new_table = table(table=self.table_mapping[table_name], quoted=False, alias=table_alias)\n",
    "                tbl.replace(new_table)\n",
    "        \n",
    "        for col in query_ast.find_all(exp.Column):\n",
    "            column_name = col.this.this\n",
    "            if column_name in self.col_mapping:\n",
    "                col.this.set('this',self.col_mapping[column_name])\n",
    "        \n",
    "        return query_ast.sql(dialect='snowflake')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29adcbbd-c65d-4676-b7a0-9bcf4a8f6dab",
   "metadata": {},
   "source": [
    "## RUN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e08a776a-7dfa-481e-b7e7-cc90e3253987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getModelResult(schema, question, model_name, selected_table, table_columns):\n",
    "    embedding_model_name = 'mixedbread-ai/mxbai-embed-large-v1'\n",
    "    try:\n",
    "        print('Running pre-processing...')\n",
    "        pruned_schema = preprocess_table(question=question, schema=schema, table_name=selected_table)\n",
    "        print(pruned_schema)\n",
    "    except Exception as e:\n",
    "        print('Pre-processing failed!')\n",
    "        print(e, e.__traceback__)\n",
    "        pruned_schema = schema\n",
    "    \n",
    "    prompt_template = \"\"\" \n",
    "                        ### Instructions:\n",
    "                        Your task is to convert a question into a SQL query, given a Postgres database schema.\n",
    "                        Adhere to these rules:\n",
    "                        - **Deliberately go through the question and database schema word by word** to appropriately answer the question\n",
    "                        - **Use Table Aliases** to prevent ambiguity. For example, `SELECT table1.col1, table2.col1 FROM table1 JOIN table2 ON table1.id = table2.id`.\n",
    "                        - When creating a ratio, always cast the numerator as float\n",
    "                        \n",
    "                        ### Input:\n",
    "                        Generate a SQL query that answers the question {question}.\n",
    "                        This query will run on a database whose schema is represented in this string:\n",
    "                        {db_schema}\n",
    "                        \n",
    "                        \n",
    "                        ### Response:\n",
    "                        Based on your instructions, here is the SQL query I have generated to answer the question {question}:\n",
    "                        ```sql\n",
    "                        \"\"\"\n",
    "\n",
    "    prompt = prompt_template.format(question=question, db_schema=pruned_schema)\n",
    "    print(f'prompt: {prompt}')\n",
    "    print(f'Querying {model_name}...')\n",
    "    ollama = OLLAMA(OLLAMA_URL=OLLAMA_URL, model_name='sqlcoder')\n",
    "    generated_query = ollama.run(prompt)\n",
    "\n",
    "    if 'i do not know' in generated_query.lower():\n",
    "        print('Failed to get SQL from model')\n",
    "        return generated_query, prompt\n",
    "    print(f'Done! Generated query: {generated_query}')\n",
    "\n",
    "    print('Running post-processing...')\n",
    "    try:\n",
    "        qp = queryPostprocessing(generated_query, {'table_name':selected_table, 'columns':table_columns}, embedding_model_name)\n",
    "        processed_query = qp.formatQuerySQLglot()\n",
    "        processed_query = processed_query.replace('\"\"', '\"')\n",
    "    except Exception as e:\n",
    "        print('Post-processing failed!')\n",
    "        print(e, e.__traceback__)\n",
    "        processed_query = generated_query\n",
    "    print(f'Done! Processed query: {processed_query}')\n",
    "    return processed_query, prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edc3ee9-ede6-4a53-9f44-9f205a044677",
   "metadata": {},
   "source": [
    "### Step 1: load sample data (kaggle titanic dataset) into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d4a6ee8-31b3-4e75-9dad-a8444bdf07ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Load Successfull\n"
     ]
    }
   ],
   "source": [
    "db = create_engine('sqlite:///mysqlitedb.db')\n",
    "df = pd.read_csv('data/titanic.csv')\n",
    "try:\n",
    "    df.to_sql('titanic', db, index=False)\n",
    "    print(f'Data Load Successfull')\n",
    "except Exception as e:\n",
    "    print(f'Failed to load CSV into SQLite DB. Error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792977c-dc22-4322-b13b-55ca27e035fd",
   "metadata": {},
   "source": [
    "### Step 2: get table description and DDL from database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce48a532-94f6-4207-a4ec-b479ff59b0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_desc_dfs = getTableDescriptionSQLiteDB(output_fmt='df')\n",
    "table_desc_ddls = getTableDescriptionSQLiteDB(output_fmt='ddl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ec3cdf6-1117-46e7-872c-accd1f78f9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"PassengerId\"</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Survived\"</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Pclass\"</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Name\"</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"Sex\"</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"Age\"</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"SibSp\"</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"Parch\"</td>\n",
       "      <td>BIGINT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"Ticket\"</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Fare\"</td>\n",
       "      <td>FLOAT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"Cabin\"</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"Embarked\"</td>\n",
       "      <td>TEXT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             name    type  comment\n",
       "0   \"PassengerId\"  BIGINT      NaN\n",
       "1      \"Survived\"  BIGINT      NaN\n",
       "2        \"Pclass\"  BIGINT      NaN\n",
       "3          \"Name\"    TEXT      NaN\n",
       "4           \"Sex\"    TEXT      NaN\n",
       "5           \"Age\"   FLOAT      NaN\n",
       "6         \"SibSp\"  BIGINT      NaN\n",
       "7         \"Parch\"  BIGINT      NaN\n",
       "8        \"Ticket\"    TEXT      NaN\n",
       "9          \"Fare\"   FLOAT      NaN\n",
       "10        \"Cabin\"    TEXT      NaN\n",
       "11     \"Embarked\"    TEXT      NaN"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_desc_dfs['titanic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "48eea632-6fcf-413b-b34b-52cdcc7a01c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CREATE TABLE titanic (\n",
      "\t\"PassengerId\" BIGINT, \n",
      "\t\"Survived\" BIGINT, \n",
      "\t\"Pclass\" BIGINT, \n",
      "\t\"Name\" TEXT, \n",
      "\t\"Sex\" TEXT, \n",
      "\t\"Age\" FLOAT, \n",
      "\t\"SibSp\" BIGINT, \n",
      "\t\"Parch\" BIGINT, \n",
      "\t\"Ticket\" TEXT, \n",
      "\t\"Fare\" FLOAT, \n",
      "\t\"Cabin\" TEXT, \n",
      "\t\"Embarked\" TEXT\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(table_desc_ddls['titanic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e4396f-3f43-4d3d-ac50-07c3e6af939f",
   "metadata": {},
   "source": [
    "### Step 3: Define business query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "525c74aa-d407-4238-afdc-b69f823c12c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"give minimum and maximum age of gender 'male' passengers\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c05cc8-1a35-4f80-8df2-b6a2a88fda2f",
   "metadata": {},
   "source": [
    "### Step 4: Preprocess schema, create prompt, run model to generate query, and postprocess the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b187e0-7a1f-4e88-8503-a708a16307ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b60f7795-1ad7-47ef-ad9b-5715bf4d594d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pre-processing...\n",
      "Pruned schema: \n",
      "CREATE TABLE titanic (\n",
      "  PassengerId BIGINT, --None\n",
      "  Age FLOAT, --None\n",
      "  Pclass BIGINT, --None\n",
      "  Fare FLOAT, --None\n",
      "  Sex TEXT, --None\n",
      "  Ticket TEXT, --None\n",
      "  SibSp BIGINT, --None\n",
      "  Parch BIGINT, --None\n",
      ");\n",
      "\n",
      "\n",
      "\n",
      "CREATE TABLE titanic (\n",
      "  PassengerId BIGINT, --None\n",
      "  Age FLOAT, --None\n",
      "  Pclass BIGINT, --None\n",
      "  Fare FLOAT, --None\n",
      "  Sex TEXT, --None\n",
      "  Ticket TEXT, --None\n",
      "  SibSp BIGINT, --None\n",
      "  Parch BIGINT, --None\n",
      ");\n",
      "\n",
      "\n",
      "prompt:  \n",
      "                        ### Instructions:\n",
      "                        Your task is to convert a question into a SQL query, given a Postgres database schema.\n",
      "                        Adhere to these rules:\n",
      "                        - **Deliberately go through the question and database schema word by word** to appropriately answer the question\n",
      "                        - **Use Table Aliases** to prevent ambiguity. For example, `SELECT table1.col1, table2.col1 FROM table1 JOIN table2 ON table1.id = table2.id`.\n",
      "                        - When creating a ratio, always cast the numerator as float\n",
      "\n",
      "                        ### Input:\n",
      "                        Generate a SQL query that answers the question give minimum and maximum age of gender 'male' passengers.\n",
      "                        This query will run on a database whose schema is represented in this string:\n",
      "                        \n",
      "CREATE TABLE titanic (\n",
      "  PassengerId BIGINT, --None\n",
      "  Age FLOAT, --None\n",
      "  Pclass BIGINT, --None\n",
      "  Fare FLOAT, --None\n",
      "  Sex TEXT, --None\n",
      "  Ticket TEXT, --None\n",
      "  SibSp BIGINT, --None\n",
      "  Parch BIGINT, --None\n",
      ");\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                        ### Response:\n",
      "                        Based on your instructions, here is the SQL query I have generated to answer the question give minimum and maximum age of gender 'male' passengers:\n",
      "                        ```sql\n",
      "                        \n",
      "Querying sqlcoder...\n",
      "JSON resp:  SELECT MIN(age) AS min_age, MAX(age) AS max_age FROM titanic WHERE sex = 'male';\n",
      "                        ```\n",
      "Done! Generated query:  SELECT MIN(age) AS min_age, MAX(age) AS max_age FROM titanic WHERE sex = 'male';\n",
      "                        ```\n",
      "Running post-processing...\n",
      "Done! Processed query: SELECT MIN(\"Age\") AS min_age, MAX(\"Age\") AS max_age FROM titanic WHERE \"Sex\" = 'male'\n"
     ]
    }
   ],
   "source": [
    "table_names = list(table_desc_ddls.keys())[0]\n",
    "schemas_df = list(table_desc_ddls.values())[0]\n",
    "table_columns = list(table_desc_dfs.values())[0]\n",
    "model_name = 'sqlcoder'\n",
    "\n",
    "sql_query, prompt = getModelResult(schemas_df, question, model_name, table_names, table_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57738374-1cf7-4ec1-8392-309167fc1bc4",
   "metadata": {},
   "source": [
    "### Step 5: Execute the post processed query against database to fetch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02060325-cb9e-447e-bef7-d17c5e891f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_age</th>\n",
       "      <th>max_age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.42</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_age  max_age\n",
       "0     0.42     80.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getSQLiteDBQueryResult(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75af00c2-cc87-4ff3-9143-f932c69dd2c2",
   "metadata": {},
   "source": [
    "## Note"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7650216-8092-4dc4-a20c-46a66794641f",
   "metadata": {},
   "source": [
    "1- For the above query, although the table schema contained total of 12 columns, preprocessing module filered top 8 columns relevant to the query:\n",
    "\n",
    "**Original Schema:** \\\n",
    "CREATE TABLE titanic (      \\\n",
    "\t\"PassengerId\" BIGINT, \\\n",
    "\t\"Survived\" BIGINT, \\\n",
    "\t\"Pclass\" BIGINT, \\\n",
    "\t\"Name\" TEXT, \\\n",
    "\t\"Sex\" TEXT, \\\n",
    "\t\"Age\" FLOAT, \\\n",
    "\t\"SibSp\" BIGINT, \\ \n",
    "\t\"Parch\" BIGINT, \\\n",
    "\t\"Ticket\" TEXT, \\\n",
    "\t\"Fare\" FLOAT, \\\n",
    "\t\"Cabin\" TEXT, \\\n",
    "\t\"Embarked\" TEXT\\\n",
    ")\n",
    "\n",
    "**Preprocesed Schema:** \\\n",
    "CREATE TABLE titanic (   \\\n",
    "  PassengerId BIGINT, --None\\\n",
    "  Age FLOAT, --None\\\n",
    "  Pclass BIGINT, --None\\\n",
    "  Fare FLOAT, --None\\\n",
    "  Sex TEXT, --None\\\n",
    "  Ticket TEXT, --None\\\n",
    "  SibSp BIGINT, --None\\\n",
    "  Parch BIGINT, --None\\\n",
    ")\n",
    "\n",
    "This helped to reduce redundant context, which results in better runtime performance and reduced hallucinations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa1e8db-f5c3-4bd8-bf75-701ff236aeec",
   "metadata": {},
   "source": [
    "2- Post processing did not alter the query as it was already correct. But here is a demonstration of how it would have fixed the query for hallucinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0dbd08bb-c1e3-4737-8aa7-a0cd0b74cac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT MIN(Age) AS minimum_age, MAX(Age) AS maximum_age FROM titanic WHERE \"sex\" = 'male'\n"
     ]
    }
   ],
   "source": [
    "incorrect_query = \"SELECT min(Age) minimum_age, max(Age) maximum_age from titanic_ship where gender = 'male'\"\n",
    "\n",
    "qp = queryPostprocessing(incorrect_query, {'table_name':table_names, 'columns':table_columns}, embedding_model_name = 'all-MiniLM-L6-v2')\n",
    "processed_query = qp.formatQuerySQLglot()\n",
    "processed_query = processed_query.replace('\"\"', '\"')\n",
    "print(processed_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b0021-7a6a-4400-82eb-20cd2c3563ec",
   "metadata": {},
   "source": [
    "During post processing, following fixes were done to the query:\n",
    "1. incorrect table name \"titanic_ship\" was imputed with correct table name \"titanic\" from schema\n",
    "2. incorrect column name \"gender\" was imputed with \"sex\" using embedding similarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
